# Rede Neural Perceptron Multicamadas (MLP)

## Construtor da rede

A rede √© iniciada com um list, onde cada elemento representar√° uma camada com n neur√¥nios

    self.num_layers (quantidade de camadas)
    self.sizes (neuronios por camada)

**ser√° tomada por exemplo [784, 30, 10]**

### Bias
Os bias (vieses) ser√£o iniciados ap√≥s a 1¬∞ camada, pois nesta apenas as entradas (dados) as alimentam.

    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]

Partiremos do indice 1 em diante, seguindo uma *distribui√ß√£o normal* (m√©dia 0, desvio padr√£o 1), criando para cada camada **um vetor de y linhas e 1 coluna** com valores aleat√≥rios.

O resultado ser√° uma lista de *arrays NumPy*, onde cada array-i representa os vieses da camada-i. Desse modo, antes de aplicar a fun√ß√£o de ativa√ß√£o o bias √© adicionado:

![Exemplo](https://imgur.com/ogXGvRL.png)

Isso permite que o neur√¥nio "desligue" ou "ligue" com mais facilidade, independentemente das entradas

**Exemplo**
* Para a camada oculta (30 neur√¥nios): um array (30, 1) com 30 vieses.
* Para a camada de sa√≠da (10 neur√¥nios): um array (10, 1) com 10 vieses.*

### Pesos
Iniciar cada peso entre as conex√µes dos neur√¥nios. Haver√° uma rela√ß√£o direta entra as camadas, porque cada neur√¥nio da camada-k dever√° se conectar com os da camada seguinte, at√© a camada de sa√≠da.

    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

O loop vai percorrer a lista de camadas, pareando (x, y) elas at√© a sa√≠da, gerando as conex√µes com pesos. Cada par cria uma matriz de ***y linhas*** e ***x colunas*** com n√∫meros aleat√≥rios da distribui√ß√£o normal.  

> y: N√∫mero de neur√¥nios na camada de destino.

> x: N√∫mero de neur√¥nios na camada de origem.

**Exemplo**

* Primeiro par: (784, 30) - Conex√£o da camada de entrada (784 neur√¥nios) para a camada oculta (30 neur√¥nios).

* Segundo par: (30, 10) - Conex√£o da camada oculta (30 neur√¥nios) para a camada de sa√≠da (10 neur√¥nios).


Cada elemento ***Wjk*** em uma matriz de pesos representa a for√ßa da conex√£o do ***k-√©simo neur√¥nio na camada anterior*** para o ***j-√©simo neur√¥nio na camada atual***.

**ùë§ùëóùëò : peso da conex√£o entrada k ‚Üí oculto j**

### Resumo da Inicializa√ß√£o

A rede √© "montada" com sua estrutura definida e seus par√¢metros (pesos e vieses) s√£o iniciados aleatoriamente. Essa inicializa√ß√£o aleat√≥ria √© crucial para quebrar a simetria e permitir que os neur√¥nios aprendam padr√µes diferentes.


## feedforward(self, a): Propagando a Entrada

Aqui recebemos um **vetor de ativa√ß√£o (a)**, que ser√° a entrada da rede. Para simular a propaga√ß√£o pela rede n√≥s *pareamos* os **biases(b)** e os **pesos entre camadas(w)**, faremos o **dot_product** entre (**w**, **a**).

    for b, w in zip(self.biases, self.weights):
        a = sigmoid(np.dot(w, a)+b)

Como **w** √© um vetor (y, x) e **a** √© (x, 1) teremos um vetor resultando (y, 1), e somando **b** da camada atual ao vetor resultante e aplicando a **fun√ß√£o de ativa√ß√£o sigmoide**, vamos atualizar **a** para ser usado como entrada da pr√≥xima camada da rede. Assim teremos a propaga√ß√£o das entradas por toda a rede.



## Treinamento com Descida de Gradiente Estoc√°stico

Aqui temos a principal fun√ß√£o para treinamento da rede, pois implementa a descida do gradiente.

    sgd(self, training_data, epochs, mini_batch_size, eta, test_data=None)

`training_data` ‚áí lista de tuplas **(x, y)**, onde **x** √© o **vetor de entrada** e **y** o **vetor de sa√≠da** esperado para **x**.

`epochs` ‚áí vezes em que a rede passar√° pelo conjunto de dados de treino.

`mini_batch_size` ‚áí quantidade de elementos nos subconjuntos para calculo do SGD.

`eta` ‚áí taxa de aprendizagem (*Œ∑*), hiperpar√¢metro crucial para definir o quanto os pesos ser√£o ajustados a cada *√©poca de treino*.

> Com um *Œ∑* muito grande, a rede pode pular o m√≠nimo e n√£o convergir. Se for muito pequeno, o treinamento ser√° lento e pode ficar preso em solu√ß√µes ruins.

Em cada √©poca, os dado de treino s√£o embaralhados antes de serem criados/separados os `mini_batchs` ou `mini_lotes`. Esse processo evita a repeti√ß√£o de dos lotes em cada √©poca e consequentemente aprendizagem viciada.

### Mini Lotes
A cria√ß√£o desse lotes √© feita atrav√©s de slices da lista training_data. Faremos da vari√°vel 
`mini_batches` uma lista de "fatias" com  `len(mini_batches[i]) == mini_batch_size`.

    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]

Desse modo o o treinamento ser√° acelerado e ajuda a evitar m√≠nimos locais:

üöÄ 1. Menos c√°lculos por passo (mais r√°pido por itera√ß√£o)

>Treinar com todos os dados de uma vez (batch gradient descent) √© muito pesado computacionalmente

üîÅ 2. Mais atualiza√ß√µes por √©poca

>Em vez de esperar passar por todos os dados para atualizar os pesos, o modelo atualiza os pesos v√°rias vezes por √©poca ‚Äî a cada mini-batch - aprendendo mais r√°pido.

üé≤ 3. Introduz aleatoriedade √∫til (melhor generaliza√ß√£o)

> Como cada mini-batch cont√©m dados diferentes, o gradiente calculado √© uma estimativa ruidosa do gradiente real. Esse "ru√≠do" ajuda a evitar m√≠nimos locais ruins e melhora a capacidade de generaliza√ß√£o da rede


### Resumo

O SGD √© um processo iterativo. Em cada √©poca, ele embaralha os dados, os divide em mini-lotes, e para cada mini-lote, ele ajusta os pesos e vieses da rede. Isso continua por v√°rias √©pocas, permitindo que a rede "aprenda" os padr√µes nos dados e melhore sua capacidade de fazer previs√µes.

## Atualizando Pesos e Vieses
    update_mini_batch(self, mini_batch, eta)

√â respons√°vel por aplicar o algoritmo de `backpropagation` para calcular os gradientes e ent√£o atualizar os **pesos** e **vieses** da rede


### ‚àá (nabla) 

Representa o gradiente de uma fun√ß√£o - em rela√ß√£o aos *pesos* (nabla_w) e aos *biases* (nabla_b) e ser√£o usados para acumular os gradientes.

    nabla_b = [np.zeros(b.shape) for b in self.biases]

### backprop nos batchs

Iremos ent√£o iterar sobre o `mini_batch` pareando entrada e sa√≠da e realizar o "*backprop*". Isso nos retornar√° os **gradientes para os biases e pesos** como listas.

    for x, y in mini_batch:
        delta_nabla_b, delta_nabla_w = self.backprop(x, y)

Para **cada par** desses ser√£o calculados os **gradientes parciais** de biases e pesos ‚Äî ou seja, **quanto cada peso/vies deveria mudar para aquele exemplo espec√≠fico**.

Eles ser√£o somados a cada itera√ß√£o (entrada) para obtermos um **gradiente m√©dio** daquele batch, suavizando as mudan√ßas que viram na atualiza√ß√£o dos par√¢metros. 


### atualizando...
    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]
    
Esse passo √© feito atrav√©s da regra delta:

‚Äã![regra delta](https://imgur.com/DmT49Fp.png)
 
Por usamos o **gradiente m√©dio** do batch, o `eta` √© dividido pelo tamanho do mini-lote, fazendo com que a atualiza√ß√£o reflita a m√©dia dos gradientes dos exemplos. Assim temos:

‚Äã![regra delta do gradiente m√©dio](https://imgur.com/8MsBzgJ.png)






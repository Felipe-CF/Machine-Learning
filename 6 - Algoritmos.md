# Naive Bayes, M√°quina de vetor de suporte (SVM), Regress√£o log√≠stica, Aprendizagem baseada em inst√¢ncias (KNN), √Årvores de decis√£o (decision tree), Random Forest

> Resultados usando a base heart_tratado.csv

Bayes = 
SVM =
Regress√£o log√≠stica =
KNN = Acur√°cia: 89.39% , Acertos: 276, 

## Naive Bayes 

CLassificador probabil√≠stico baseado no teorema de Bayes.


![Formula](https://imgur.com/kCo5g2Y.jpg)


**Premissa**: independ√™ncia entre as vari√°veis

**Obs**: trabalha bem com vari√°veis categ√≥ricas

### Vantagens
√â de f√°cil entendimento, pouco esfor√ßo computacional, bom desempenho com muitos dados e previs√µes com poucos dados.

### Desvantagens
Consiedrar atributos independentes, **atribui nula probabilidade** quando uma classe contida no teste n√£o se apresente no treino.


### C√≥digo

    x_train, x_test, y_train, y_test = train_test_split(previsores3, alvo, random_state=0, test_size=0.3) 

    naive = GaussianNB()

    naive.fit(x_test, y_test)

    previsoes_teste = naive.predict(x_test)

***arquivo: bayes.py***


## M√°quina de vetor de suporte (SVM)

Aplicado em problemas de aprendizagem supervisionada tanto de classifica√ß√£o como de regress√£o. **Em classifica√ßao** √© conhecido como ***CLassificador de Vetor de Suporte*** (SVC). O objetivo √© **criar hiperplanos de separa√ß√£o dos dados**, podendo ser aplicado em *problemas linearmente e n√£o linearmente separ√°veis*.

![SVM](https://imgur.com/ejRktq9.jpg)

### Equa√ß√£o

> w * x + b >= 0

* w: 
* x: 
* b: 

### Aplica√ß√µes

* Classifica√ß√£o
* Categoriza√ß√£o de textos
* Reconhecimento de imagem e letras manuscritas
* Detec√ß√£o facial e de anomalias

### Constante de penaliza√ß√£o (custo)

> Os **hiperpar√¢metros** s√£o aqueles que √© poss√≠vel alterar/ajustar para conseguir um melhor resultado no algortimo. 

**Hiperpar√¢metro C**: controla a toler√¢ncia dos erros.

Quanto **maior o valor de C**, maior o poder de separa√ß√£o das classes, por√©m maior a probabilidade de *overfitting* e do *tempo de treinamento*. Qu√£o **menor for o valor de C**, aumenta a chance de erros na sepra√ß√£o e *underfitting*.

O hiperpar√¢metro **gamma** pode ser ajustado para otimiza√ß√£o.

### Vantagens

* n√£o √© influenciado por dados discrepantes
* solu√ß√£o de problemas lineares e n√£o lineares
* efetivo em datasets grandes
* consegue aprender com caracter√≠sticas n√£o pertencentes aos dados

### Desvantagens

* dif√≠cil visualiza√ß√£o gr√°fica e interpreta√ß√£o te√≥rica
* mais lento em compara√ß√£o a outros
* cuidado com os hiperpar√¢metros para evitar *overfitting* e *underfitting*


### c√≥digo

'''
from sklearn.svm import SVC

svm = SVC(kernel='rbf', random_state=1, C=2)

svm.fit(x_train, y_train)
'''

## Regress√£o log√≠stica

Recebe esse nome devido a utiliza√ß√£o de conceitos de **regress√£o linear** em seu modelo matem√°tico. Ela pode ser **usada em problemas bin√°rios**, vari√°vel dependente bin√°ria (duas sa√≠das),  **ou multinomila** (vari√°vel dependente com mais de duas categorias)

![Formula](https://imgur.com/KxTBsV7.jpg)

* p= probabilidade de pertencer a determinada classe
* e= n√∫mero de Euler
* b0= intercepto
* bn= coeficientes
* xn= vari√°veis dependentes

### Comportamento

A **regress√£o linear** tende a extrapolar os limites pois: ùë• ‚Üí ¬±‚àû,  y ‚Üí ¬±‚àû. Isso √© problem√°tico quando se trata de probabilidades (entre 0 e 1). J√° a **log√≠stica** limita a sa√≠da a: 0 < y < 1. Ela √© ideal para **prever probabilidade de eventos bin√°rios** (sim ou n√£o).

![Compara√ß√£o entre as regress√µes](https://imgur.com/OB1uQ53.jpg)

O **limiar de decis√£o** √© usado para escolher/definir o resultado, pois caso o **valor esteja acima**, a **sa√≠da ser√° 1**, **se for menor ser√° 0**.

### Vantagens

* f√°cil implementa√ß√£o
* teoria consolidada
* excelente desempenho
* indica o valor de probabilidade para cada inst√¢ncia 

### C√≥digo e Par√¢metros

* C = controla a toler√¢ncia dos erros (separa as classes).
* penalty = evita overfitting, multicolinearidade
* max_iter = evita overfitting e torna o modelo mais simples e generaliz√°vel definindo limite de itera√ß√µes que o algoritmo ter√° para melhorar o resultado 
* solver = algoritmo que busca otimiza√ß√£o e menor erro poss√≠vel (ligado com o 'penalty')
* max_iter = limite de itera√ß√µes que o algoritmo ter√° para melhorar o resultado
* tol = trabalha junto com 'max_iter' definindo o limite de erros


## Aprendizagem baseada em inst√¢ncias (KNN)

Define agrupamento nos dados e, ao chegar bivis dados, tra√ßa uma √°rea ao redor dele para identificar a que grupo ele pertence.

![KNN](https://imgur.com/5TKGFoh.jpg)


## √Årvores de decis√£o (decision tree)

Aplicado em problemas de aprendizagem supervisionada tanto de **classifica√ß√£o (mais utilizado)** como de **regress√£o**.

Seleciona a ordem que os atributos ir√£o aparecer na √°rvore, *sempre de cima para baixo*, conforme sua *import√¢ncia para a predi√ß√£o*, assim como determina a separa√ß√£o dos ramos da √°rvore.

![Exemplo](https://imgur.com/j0LO8iV.jpg)

### Podagem

Objetiva diminuir a probabilidade de overfitting.
Pode ser de duas formas:

1) Pr√©-podagem: parar o crescimento da √°rvore.
2) P√≥s-podagem: poda com a √°rvore j√° completa.

**Processo de podagem**:
- Percorre a √°rvore em profundidade.
- Para cada n√≥ de decis√£o calcula o erro no n√≥ e a soma dos erros nos n√≥s descendentes.
- Se o erro do n√≥ √© menor ou igual √† soma dos erros dos
n√≥s descendentes ent√£o o n√≥ √© transformado em folha.



## Random Forest

Cria√ß√£o aleat√≥ria de v√°rias √°rvores de decis√£o.

Utiliza o m√©todo Ensemble (constru√ß√£o de v√°rios modelos para obter um resultado √∫nico).

√â mais robusto, complexo e normalmente propicia resultados melhores, mas possui maior custo computacional.

Em *problemas de classifica√ß√£o* o resultado que *mais aparece* ser√° o escolhido (moda), j√° em **regress√£o ser√° a m√©dia**.

![Ilustra√ß√£o](https://imgur.com/K41BnRM.jpg)

### Diferen√ßas referente a √Årvore de decis√£o

|√Årvore de decis√£o|Random Forest|
|---|---|
|apenas uma √°rvore|conjunto de √°rvores.|
|cria regras para sele√ß√£o das melhores vari√°veis|sele√ß√£o das vari√°veis aleatoriamente.|
|resultado √© ‚Äúfruto‚Äù de uma √∫nica √°rvore|resultado √© a moda ou m√©dia de todas as √°rvores.|

### Vantagens X Desvantagens

|Vantagens|Desvantagens|
|---|---|
|Resultados bastante precisos.|Velocidade de processamento relativamente baixa|
|Normalmente n√£o necessitam de prepara√ß√µes sofisticadas nos dados (label Encoder e OneHot Encoder)|Dif√≠cil interpreta√ß√£o de como chegou no resultado|
|Trabalha com valores faltantes, vari√°veis categ√≥ricas e num√©ricas.|---|
|Pouca probabilidade de ocorr√™ncia de overfitting.|---|



# Rede Neural Perceptron Multicamadas (MLP)

## Construtor da rede

A rede √© iniciada com um list, onde cada elemento representar√° uma camada com n neur√¥nios

    self.num_layers (quantidade de camadas)
    self.sizes (neuronios por camada)

**ser√° tomada por exemplo [784, 30, 10]**

### Bias
Os bias (vieses) ser√£o iniciados ap√≥s a 1¬∞ camada, pois nesta apenas as entradas (dados) as alimentam.

    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]

Partiremos do indice 1 em diante, seguindo uma *distribui√ß√£o normal* (m√©dia 0, desvio padr√£o 1), criando para cada camada **um vetor de y linhas e 1 coluna** com valores aleat√≥rios.

O resultado ser√° uma lista de *arrays NumPy*, onde cada array-i representa os vieses da camada-i. Desse modo, antes de aplicar a fun√ß√£o de ativa√ß√£o o bias √© adicionado:

![Exemplo](https://imgur.com/ogXGvRL.png)

Isso permite que o neur√¥nio "desligue" ou "ligue" com mais facilidade, independentemente das entradas

**Exemplo**
* Para a camada oculta (30 neur√¥nios): um array (30, 1) com 30 vieses.
* Para a camada de sa√≠da (10 neur√¥nios): um array (10, 1) com 10 vieses.*

### Pesos
Iniciar cada peso entre as conex√µes dos neur√¥nios. Haver√° uma rela√ß√£o direta entra as camadas, porque cada neur√¥nio da camada-k dever√° se conectar com os da camada seguinte, at√© a camada de sa√≠da.

    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

O loop vai percorrer a lista de camadas, pareando (x, y) elas at√© a sa√≠da, gerando as conex√µes com pesos. Cada par cria uma matriz de ***y linhas*** e ***x colunas*** com n√∫meros aleat√≥rios da distribui√ß√£o normal.  

> y: N√∫mero de neur√¥nios na camada de destino.

> x: N√∫mero de neur√¥nios na camada de origem.

**Exemplo**

* Primeiro par: (784, 30) - Conex√£o da camada de entrada (784 neur√¥nios) para a camada oculta (30 neur√¥nios).

* Segundo par: (30, 10) - Conex√£o da camada oculta (30 neur√¥nios) para a camada de sa√≠da (10 neur√¥nios).


Cada elemento ***Wjk*** em uma matriz de pesos representa a for√ßa da conex√£o do ***k-√©simo neur√¥nio na camada anterior*** para o ***j-√©simo neur√¥nio na camada atual***.

**ùë§ùëóùëò : peso da conex√£o entrada k ‚Üí oculto j**

### Resumo da Inicializa√ß√£o

A rede √© "montada" com sua estrutura definida e seus par√¢metros (pesos e vieses) s√£o iniciados aleatoriamente. Essa inicializa√ß√£o aleat√≥ria √© crucial para quebrar a simetria e permitir que os neur√¥nios aprendam padr√µes diferentes.


## feedforward(self, a): Propagando a Entrada

Aqui recebemos um **vetor de ativa√ß√£o (a)**, que ser√° a entrada da rede. Para simular a propaga√ß√£o pela rede n√≥s *pareamos* os **biases(b)** e os **pesos entre camadas(w)**, faremos o **dot_product** entre (**w**, **a**).

    for b, w in zip(self.biases, self.weights):
        a = sigmoid(np.dot(w, a)+b)

Como **w** √© um vetor (y, x) e **a** √© (x, 1) teremos um vetor resultando (y, 1), e somando **b** da camada atual ao vetor resultante e aplicando a **fun√ß√£o de ativa√ß√£o sigmoide**, vamos atualizar **a** para ser usado como entrada da pr√≥xima camada da rede. Assim teremos a propaga√ß√£o das entradas por toda a rede.



## Treinamento com Descida de Gradiente Estoc√°stico

Aqui temos a principal fun√ß√£o para treinamento da rede, pois implementa a descida do gradiente.

    sgd(self, training_data, epochs, mini_batch_size, eta, test_data=None)

`training_data` ‚áí lista de tuplas **(x, y)**, onde **x** √© o **vetor de entrada** e **y** o **vetor de sa√≠da** esperado para **x**.

`epochs` ‚áí vezes em que a rede passar√° pelo conjunto de dados de treino.

`mini_batch_size` ‚áí quantidade de elementos nos subconjuntos para calculo do SGD.

`eta` ‚áí taxa de aprendizagem (*Œ∑*), hiperpar√¢metro crucial para definir o quanto os pesos ser√£o ajustados a cada *√©poca de treino*.

> Com um *Œ∑* muito grande, a rede pode pular o m√≠nimo e n√£o convergir. Se for muito pequeno, o treinamento ser√° lento e pode ficar preso em solu√ß√µes ruins.

Em cada √©poca, os dado de treino s√£o embaralhados antes de serem criados/separados os `mini_batchs` ou `mini_lotes`. Esse processo evita a repeti√ß√£o de dos lotes em cada √©poca e consequentemente aprendizagem viciada.

### Mini Lotes
A cria√ß√£o desse lotes √© feita atrav√©s de slices da lista training_data. Faremos da vari√°vel 
`mini_batches` uma lista de "fatias" com  `len(mini_batches[i]) == mini_batch_size`.

    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]

Desse modo o o treinamento ser√° acelerado e ajuda a evitar m√≠nimos locais:

üöÄ 1. Menos c√°lculos por passo (mais r√°pido por itera√ß√£o)

>Treinar com todos os dados de uma vez (batch gradient descent) √© muito pesado computacionalmente

üîÅ 2. Mais atualiza√ß√µes por √©poca

>Em vez de esperar passar por todos os dados para atualizar os pesos, o modelo atualiza os pesos v√°rias vezes por √©poca ‚Äî a cada mini-batch - aprendendo mais r√°pido.

üé≤ 3. Introduz aleatoriedade √∫til (melhor generaliza√ß√£o)

> Como cada mini-batch cont√©m dados diferentes, o gradiente calculado √© uma estimativa ruidosa do gradiente real. Esse "ru√≠do" ajuda a evitar m√≠nimos locais ruins e melhora a capacidade de generaliza√ß√£o da rede


### Resumo

O SGD √© um processo iterativo. Em cada √©poca, ele embaralha os dados, os divide em mini-lotes, e para cada mini-lote, ele ajusta os pesos e vieses da rede. Isso continua por v√°rias √©pocas, permitindo que a rede "aprenda" os padr√µes nos dados e melhore sua capacidade de fazer previs√µes.

## Atualizando Pesos e Vieses
    update_mini_batch(self, mini_batch, eta)

√â respons√°vel por aplicar o algoritmo de `backpropagation` para calcular os gradientes e ent√£o atualizar os **pesos** e **vieses** da rede


### ‚àá (nabla) 

Representa o gradiente de uma fun√ß√£o - em rela√ß√£o aos *pesos* (nabla_w) e aos *biases* (nabla_b) e ser√£o usados para acumular os gradientes.

    nabla_b = [np.zeros(b.shape) for b in self.biases]

### backprop nos batchs

Iremos ent√£o iterar sobre o `mini_batch` pareando entrada e sa√≠da e realizar o "*backprop*". Isso nos retornar√° os **gradientes para os biases e pesos** como listas.

    for x, y in mini_batch:
        delta_nabla_b, delta_nabla_w = self.backprop(x, y)

Para **cada par** desses ser√£o calculados os **gradientes parciais** de biases e pesos ‚Äî ou seja, **quanto cada peso/vies deveria mudar para aquele exemplo espec√≠fico**.

Eles ser√£o somados a cada itera√ß√£o (entrada) para obtermos um **gradiente m√©dio** daquele batch, suavizando as mudan√ßas que viram na atualiza√ß√£o dos par√¢metros. 


### atualizando...
    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]

Esse passo √© feito atrav√©s da regra delta:

‚Äã![regra delta](https://imgur.com/DmT49Fp.png)
 
Por usamos o **gradiente m√©dio** do batch, o `eta` √© dividido pelo tamanho do mini-lote, fazendo com que a atualiza√ß√£o reflita a m√©dia dos gradientes dos exemplos. Assim temos:

‚Äã![regra delta do gradiente m√©dio](https://imgur.com/8MsBzgJ.png)


## Backpropagation

Aqui n√≥s teremos outra parte fundamental, onde iremos calcular as `derivadas parciais da fun√ß√£o de custo`, em rela√ß√£o a cada *peso* e *vi√©s*. Isso √© feito propagando o `erro da sa√≠da da rede` (√∫ltima camada) de volta para a entrada dela.

    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])

### O erro
Esta √© a "Equa√ß√£o BP1" da backpropagation (ou uma variante dela), que diz que `o erro na camada de sa√≠da` √© a **derivada do custo** em rela√ß√£o **√† ativa√ß√£o da sa√≠da**, multiplicada pela **derivada da fun√ß√£o de ativa√ß√£o**

O `erro da sa√≠da da rede` mede o `quanto quanto o resultado obtido esta distante do esperado` (y), bem como a corre√ß√£o (baseado no grandiente *sigmoid*) para que o que obtemos se aproxime do esperado. O `delta` ser√° um vetor NumPy com as mesmas dimens√µes da sa√≠da da rede.

![erro](https://imgur.com/5q2Mv4T.png)

Partida da `regra da cadeia` (*chain rule*) temos: 

![erro](https://imgur.com/0KWoxDP.png)

‚Üí Derivada da fun√ß√£o de custo em rela√ß√£o √† `ativa√ß√£o de sa√≠da` (o quanto a predi√ß√£o errou)

    self.cost_derivative(activations[-1], y)

‚Üí Derivada da fun√ß√£o de ativa√ß√£o (sigmoid, ReLU...) aplicada √† `sa√≠da da camada` (produto escalar)

    sigmoid_prime(zs[-1])


**Exemplo**

```
activations[-1] = [[0.9], [0.1], [0.8]]  # sa√≠da obtida de 3 neur√¥nios

y = [[1.0], [0.0], [1.0]] # sa√≠da esperada

self.cost_derivative(activations[-1], y) = [[-0.1], [0.1], [-0.2]]
```

üß† Intui√ß√£o da derivada de ùëé (sigmoid):
> Quando a sa√≠da a‚âà0.5, a derivada √© m√°xima (maior sensibilidade).

> Quando a‚âà0 ou a‚âà1, a derivada √© pequena (fun√ß√£o saturada).

Isso mostra o quanto um neur√¥nio pode aprender: ele aprende mais quando est√° longe dos extremos.

### Vieses
O gradiente dos vieses da √∫ltima camada √© igual ao delta da camada de sa√≠da, pois `o bias afeta linearmente a sa√≠da z` e esse gradiente, em rela√ß√£o ao bias, √© o pr√≥prio erro.

    nabla_b[-1] = delta

### Pesos
No caso dos pesos, precisamos saber como cada um deles contribuiu para o erro gerado. 

    nabla_w[-1] = np.dot(delta, activations[-2].transpose())

Como `delta` √© um vetor coluna `(m, 1)` assim como a pen√∫ltima ativa√ß√£o da rede `activations[-2]` (n, 1), n√≥s precisamos transpor esse √∫ltimo e obter `(1, n)`. Quando fizermos o `np.dot` vamos ober uma matriz `(m, n)`, tal como s√£o os pesos da rede.

### Propagando...

Esses procedimentos s√£o a base e dever√£o sofrer repeti√ß√µes ao longo da rede, mas no sentido contr√°rio a ativa√ß√£o dela (backprop). N√≥s iremos seguir o fluxo abaixo:

1. **pegar a ativa√ß√£o anterior** (*z*)
    ````
    z = zs[-l]
    ````
2. **calcular a derivada da fun√ß√£o de ativa√ß√£o referente a** *z*
    ````
     sp = sigmoid_prime(z)
    ````
3. **repetir o c√°lculo do do delta para esta camada, usando os pesos que a conectam a camada seguinte a ela (forward)**
    ````
    delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
    ````

4. **atualizar nabla_b**
    ````
    nabla_b[-l] = delta
    ````

5. **atualizar nabla_w**
    ````
    nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
    ````





![alt text](image-1.png)